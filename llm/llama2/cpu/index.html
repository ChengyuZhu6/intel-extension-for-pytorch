<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<meta content="This website introduces Intel® Extension for PyTorch*" name="description" />
<meta content="Intel optimization, PyTorch, Intel® Extension for PyTorch*, LLM" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Intel® Extension for PyTorch* Large Language Model (LLM) Feature Get Started &mdash; intel_extension_for_pytorch 2.1.0.dev+cpu.llm documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            intel_extension_for_pytorch
          </a>
              <div class="version">
                2.1.0.dev+cpu.llm
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Intel® Extension for PyTorch* Large Language Model (LLM) Feature Get Started</a><ul>
<li><a class="reference internal" href="#system-requirements">System Requirements</a></li>
<li><a class="reference internal" href="#installation">Installation</a><ul>
<li><a class="reference internal" href="#install-from-prebuilt-wheel-files">Install From Prebuilt Wheel Files</a></li>
<li><a class="reference internal" href="#compile-from-source">Compile From Source</a></li>
</ul>
</li>
<li><a class="reference internal" href="#launch-examples">Launch Examples</a><ul>
<li><a class="reference internal" href="#supported-models">Supported Models</a></li>
<li><a class="reference internal" href="#install-dependencies">Install Dependencies</a></li>
<li><a class="reference internal" href="#run-examples">Run Examples</a><ul>
<li><a class="reference internal" href="#preparations">Preparations</a></li>
<li><a class="reference internal" href="#single-instance-performance">Single Instance Performance</a></li>
<li><a class="reference internal" href="#single-instance-accuracy">Single Instance Accuracy</a></li>
<li><a class="reference internal" href="#distributed-performance-with-deepspeed-autotp">Distributed Performance with DeepSpeed (autoTP)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Intel® Extension for PyTorch* Large Language Model (LLM) Feature Get Started</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="intel-extension-for-pytorch-large-language-model-llm-feature-get-started">
<h1>Intel® Extension for PyTorch* Large Language Model (LLM) Feature Get Started<a class="headerlink" href="#intel-extension-for-pytorch-large-language-model-llm-feature-get-started" title="Permalink to this heading"></a></h1>
<p>Intel® Extension for PyTorch* extends optimizations to large language models (LLM). Optimizations are at development and experimental phase at this moment. You are welcomed to have a try with these optimizations on 4th Gen Intel® Xeon® Scalable processors.</p>
<section id="system-requirements">
<h2>System Requirements<a class="headerlink" href="#system-requirements" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><th class="stub"><p>Hardware</p></th>
<td><p>4th Gen Intel® Xeon® Scalable processors</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>OS</p></th>
<td><p>CentOS/RHEL 8</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Linux Kernel</p></th>
<td><p>Intel® 4th Gen Xeon® Platinum: 5.15.0; Intel® 4th Gen Xeon® Max: 5.19.0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Python</p></th>
<td><p>3.9, conda is required.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Compiler</p></th>
<td><p>Preset in the compilation script below, if compile from source</p></td>
</tr>
</tbody>
</table>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading"></a></h2>
<p>Prebuilt wheel file are available for Python 3.9. Alternatively, a script is provided to compile from source.</p>
<section id="install-from-prebuilt-wheel-files">
<h3>Install From Prebuilt Wheel Files<a class="headerlink" href="#install-from-prebuilt-wheel-files" title="Permalink to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">https://download.pytorch.org/whl/nightly/cpu/torch-2.1.0.dev20230711%2Bcpu-cp39-cp39-linux_x86_64.whl</span><span class="w"> </span><span class="nv">https://download.pytorch.org/whl/nightly/cpu/torchvision-0.16.0.dev20230711%2Bcpu-cp39-cp39-linux_x86_64.whl</span><span class="w"> </span><span class="nv">https://download.pytorch.org/whl/nightly/cpu/torchaudio-2.1.0.dev20230711%2Bcpu-cp39-cp39-linux_x86_64.whl</span>
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.1.0.dev0%2Bcpu.llm-cp39-cp39-linux_x86_64.whl
conda<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>libstdcxx-ng<span class="o">=</span><span class="m">12</span><span class="w"> </span>-c<span class="w"> </span>conda-forge
</pre></div>
</div>
</section>
<section id="compile-from-source">
<h3>Compile From Source<a class="headerlink" href="#compile-from-source" title="Permalink to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure you are using a Python 3.9 conda environment.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/scripts/compile_bundle.sh
sed -i "18 i conda update -y sysroot_linux-64" compile_bundle.sh
sed -i "49s|.*|python -m pip install https://download.pytorch.org/whl/nightly/cpu/torch-2.1.0.dev20230711%2Bcpu-cp39-cp39-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cpu/torchvision-0.16.0.dev20230711%2Bcpu-cp39-cp39-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cpu/torchaudio-2.1.0.dev20230711%2Bcpu-cp39-cp39-linux_x86_64.whl|" compile_bundle.sh
bash<span class="w"> </span>compile_bundle.sh
</pre></div>
</div>
</section>
</section>
<section id="launch-examples">
<h2>Launch Examples<a class="headerlink" href="#launch-examples" title="Permalink to this heading"></a></h2>
<section id="supported-models">
<h3>Supported Models<a class="headerlink" href="#supported-models" title="Permalink to this heading"></a></h3>
<p>The following 3 models are supported. When running the example scripts, it is needed to replace the place holder <em>&lt;MODEL_ID&gt;</em> in example launch commands with:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><th class="stub"><p>GPT-J</p></th>
<td><p>“EleutherAI/gpt-j-6b”</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>GPT-Neox</p></th>
<td><p>“EleutherAI/gpt-neox-20b”</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Llama 2</p></th>
<td><p>Model directory path output from the <a class="reference external" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">transformers conversion tool</a>.* Verified <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-chat">meta-llama/Llama-2-7b-chat</a> and <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat">meta-llama/Llama-2-13b-chat</a>.</p></td>
</tr>
</tbody>
</table>
<p>* Llama 2 model conversion steps:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Follow <a class="reference external" href="https://github.com/facebookresearch/llama#access-on-hugging-face">instructions</a> to download model files for conversion.</p></li>
<li><p>Decompress the downloaded model file.</p></li>
<li><p>Follow <a class="reference external" href="https://github.com/facebookresearch/llama-recipes#model-conversion-to-hugging-face">instructions</a> to convert the model.</p></li>
<li><p>Launch example scripts with the place holder <em>&lt;MODEL_ID&gt;</em> substituted by the <em>--output_dir</em> argument value of the conversion script.</p></li>
</ol>
</div></blockquote>
</section>
<section id="install-dependencies">
<h3>Install Dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this heading"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>gperftools<span class="w"> </span>-c<span class="w"> </span>conda-forge
conda<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>intel-openmp
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.28.1<span class="w"> </span>cpuid<span class="w"> </span>accelerate<span class="w"> </span>datasets<span class="w"> </span>sentencepiece<span class="w"> </span><span class="nv">protobuf</span><span class="o">==</span><span class="m">3</span>.20.3

<span class="c1"># [Optional] install neural-compressor for GPT-J INT8 only</span>
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>neural-compressor<span class="o">==</span><span class="m">2</span>.2

<span class="c1"># [Optional] The following is only for DeepSpeed case</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/delock/DeepSpeedSYCLSupport
<span class="nb">cd</span><span class="w"> </span>DeepSpeedSYCLSupport
git<span class="w"> </span>checkout<span class="w"> </span>gma/run-opt-branch
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements/requirements.txt
python<span class="w"> </span>setup.py<span class="w"> </span>install
<span class="nb">cd</span><span class="w"> </span>../
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/oneapi-src/oneCCL.git
<span class="nb">cd</span><span class="w"> </span>oneCCL
mkdir<span class="w"> </span>build
<span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>..
make<span class="w"> </span>-j<span class="w"> </span>install
<span class="nb">source</span><span class="w"> </span>_install/env/setvars.sh
<span class="nb">cd</span><span class="w"> </span>../..
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If an error complaining <em>ninja</em> is not found when compiling deepspeed, please use conda and pip command to uninstall all ninja packages, and reinstall it with pip.</p>
</div>
</section>
<section id="run-examples">
<h3>Run Examples<a class="headerlink" href="#run-examples" title="Permalink to this heading"></a></h3>
<p>The following 5 python scripts are provided in Github repo <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/">example directory</a> to launch inference workloads with supported models.</p>
<ul class="simple">
<li><p>run_generation.py</p></li>
<li><p>run_generation_with_deepspeed.py</p></li>
<li><p>run_gpt-j_int8.py</p></li>
<li><p>run_gpt-neox_int8.py</p></li>
<li><p>run_llama_int8.py</p></li>
</ul>
<section id="preparations">
<h4>Preparations<a class="headerlink" href="#preparations" title="Permalink to this heading"></a></h4>
<p>A separate <em>prompt.json</em> file is required to run performance benchmarks. You can use the command below to download a sample file. For simple testing, an argument <em>--prompt</em> is provided by the scripts to take a text for processing.</p>
<p>To get these Python scripts, you can either get the entire Github repository down with git command, or use the following wget commands to get individual scripts.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the example scripts with git command</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch.git
<span class="nb">cd</span><span class="w"> </span>intel-extension-for-pytorch
git<span class="w"> </span>checkout<span class="w"> </span>v2.1.0.dev+cpu.llm
<span class="nb">cd</span><span class="w"> </span>examples/cpu/inference/python/llm

<span class="c1"># Alternatively, get individual example scripts</span>
wget<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_generation.py
wget<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
wget<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_gpt-j_int8.py
wget<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_gpt-neox_int8.py
wget<span class="w"> </span>https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_llama_int8.py

<span class="c1"># Get the sample prompt.json</span>
<span class="c1"># Make sure the downloaded prompt.json file is under the same directory as that of the python scripts mentioned above.</span>
wget<span class="w"> </span>https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json
</pre></div>
</div>
<p>The following environment variables are required to achieve a good performance on 4th Gen Intel® Xeon® Scalable processors.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">CONDA_PREFIX</span><span class="si">}</span>/lib/libstdc++.so.6

<span class="c1"># Setup environment variables for performance on Xeon</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_BLOCKTIME</span><span class="o">=</span>INF
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_TPAUSE</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_SETTINGS</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_AFFINITY</span><span class="o">=</span><span class="nv">granularity</span><span class="o">=</span>fine,compact,1,0
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_FORJOIN_BARRIER_PATTERN</span><span class="o">=</span>dist,dist
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_PLAIN_BARRIER_PATTERN</span><span class="o">=</span>dist,dist
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_REDUCTION_BARRIER_PATTERN</span><span class="o">=</span>dist,dist
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_PRELOAD</span><span class="si">}</span>:<span class="si">${</span><span class="nv">CONDA_PREFIX</span><span class="si">}</span>/lib/libiomp5.so<span class="w"> </span><span class="c1"># Intel OpenMP</span>

<span class="c1"># Tcmalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_PRELOAD</span><span class="si">}</span>:<span class="si">${</span><span class="nv">CONDA_PREFIX</span><span class="si">}</span>/lib/libtcmalloc.so
</pre></div>
</div>
</section>
<section id="single-instance-performance">
<h4>Single Instance Performance<a class="headerlink" href="#single-instance-performance" title="Permalink to this heading"></a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get prompt file to the path of scripts</span>
mv<span class="w"> </span>PATH/TO/prompt.json<span class="w"> </span>WORK_DIR

<span class="c1"># bfloat16 benchmark</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>&lt;physical<span class="w"> </span>cores<span class="w"> </span>num&gt;<span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span>&lt;node<span class="w"> </span>N&gt;<span class="w"> </span>-C<span class="w"> </span>&lt;physical<span class="w"> </span>cores<span class="w"> </span>list&gt;<span class="w"> </span>python<span class="w"> </span>run_generation.py<span class="w"> </span>--benchmark<span class="w"> </span>-m<span class="w"> </span>&lt;MODEL_ID&gt;<span class="w"> </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span>--ipex<span class="w"> </span>--jit

<span class="c1"># int8 benchmark</span>
<span class="c1">## (1) Do quantization to get the quantized model</span>
mkdir<span class="w"> </span>saved_results

<span class="c1">## GPT-J quantization</span>
python<span class="w"> </span>run_gpt-j_int8.py<span class="w"> </span>--ipex-smooth-quant<span class="w"> </span>--lambada<span class="w"> </span>--output-dir<span class="w"> </span><span class="s2">&quot;saved_results&quot;</span><span class="w"> </span>--jit<span class="w"> </span>--int8-bf16-mixed<span class="w"> </span>-m<span class="w"> </span>&lt;GPTJ<span class="w"> </span>MODEL_ID&gt;
<span class="c1">## Llama 2 quantization</span>
python<span class="w"> </span>run_llama_int8.py<span class="w"> </span>--ipex-smooth-quant<span class="w"> </span>--lambada<span class="w"> </span>--output-dir<span class="w"> </span><span class="s2">&quot;saved_results&quot;</span><span class="w"> </span>--jit<span class="w"> </span>--int8-bf16-mixed<span class="w"> </span>-m<span class="w"> </span>&lt;LLAMA<span class="w"> </span>MODEL_ID&gt;
<span class="c1">## GPT-NEOX quantization</span>
python<span class="w"> </span>run_gpt-neox_int8.py<span class="w"> </span>--ipex-weight-only-quantization<span class="w"> </span>--lambada<span class="w"> </span>--output-dir<span class="w"> </span><span class="s2">&quot;saved_results&quot;</span><span class="w"> </span>--jit<span class="w"> </span>--int8<span class="w"> </span>-m<span class="w"> </span>&lt;GPT-NEOX<span class="w"> </span>MODEL_ID&gt;

<span class="c1">## (2) Run int8 performance test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>&lt;physical<span class="w"> </span>cores<span class="w"> </span>num&gt;<span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span>&lt;node<span class="w"> </span>N&gt;<span class="w"> </span>-C<span class="w"> </span>&lt;cpu<span class="w"> </span>list&gt;<span class="w"> </span>python<span class="w"> </span>run_&lt;MODEL&gt;_int8.py<span class="w"> </span>-m<span class="w"> </span>&lt;MODEL_ID&gt;<span class="w"> </span>--quantized-model-path<span class="w"> </span><span class="s2">&quot;./saved_results/best_model.pt&quot;</span><span class="w"> </span>--benchmark<span class="w"> </span>--jit<span class="w"> </span>--int8-bf16-mixed
</pre></div>
</div>
</section>
<section id="single-instance-accuracy">
<h4>Single Instance Accuracy<a class="headerlink" href="#single-instance-accuracy" title="Permalink to this heading"></a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># bfloat16</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>&lt;physical<span class="w"> </span>cores<span class="w"> </span>num&gt;<span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span>&lt;node<span class="w"> </span>N&gt;<span class="w"> </span>-C<span class="w"> </span>&lt;physical<span class="w"> </span>cores<span class="w"> </span>list&gt;<span class="w"> </span>python<span class="w"> </span>run_generation.py<span class="w"> </span>--accuracy-only<span class="w"> </span>-m<span class="w"> </span>&lt;MODEL_ID&gt;<span class="w"> </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span>--ipex<span class="w"> </span>--jit<span class="w"> </span>--lambada

<span class="c1"># Quantization as a performance part</span>
<span class="c1"># (1) Do quantization to get the quantized model as mentioned above</span>
<span class="c1"># (2) Run int8 accuracy test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>&lt;physical<span class="w"> </span>cores<span class="w"> </span>num&gt;<span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span>&lt;node<span class="w"> </span>N&gt;<span class="w"> </span>-C<span class="w"> </span>&lt;cpu<span class="w"> </span>list&gt;<span class="w"> </span>python<span class="w"> </span>run_&lt;MODEL&gt;_int8.py<span class="w"> </span>-m<span class="w"> </span>&lt;MODEL<span class="w"> </span>ID&gt;<span class="w"> </span>--quantized-model-path<span class="w"> </span><span class="s2">&quot;./saved_results/best_model.pt&quot;</span><span class="w"> </span>--accuracy-only<span class="w"> </span>--jit<span class="w"> </span>--int8-bf16-mixed<span class="w"> </span>--lambada
</pre></div>
</div>
</section>
<section id="distributed-performance-with-deepspeed-autotp">
<h4>Distributed Performance with DeepSpeed (autoTP)<a class="headerlink" href="#distributed-performance-with-deepspeed-autotp" title="Permalink to this heading"></a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DS_SHM_ALLREDUCE</span><span class="o">=</span><span class="m">1</span>
<span class="nb">unset</span><span class="w"> </span>KMP_AFFINITY

<span class="c1"># Get prompt file to the path of scripts</span>
mv<span class="w"> </span>PATH/TO/prompt.json<span class="w"> </span>WORK_DIR

<span class="c1"># Run GPTJ/LLAMA with bfloat16  DeepSpeed</span>
deepspeed<span class="w"> </span>--bind_cores_to_rank<span class="w"> </span>run_generation_with_deepspeed.py<span class="w"> </span>--benchmark<span class="w"> </span>-m<span class="w"> </span>&lt;MODEL_ID&gt;<span class="w"> </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span>--ipex<span class="w"> </span>--jit

<span class="c1"># Run GPT-NeoX with ipex weight only quantization</span>
deepspeed<span class="w"> </span>--bind_cores_to_rank<span class="w"> </span>run_generation_with_deepspeed.py<span class="w"> </span>--benchmark<span class="w"> </span>-m<span class="w"> </span>EleutherAI/gpt-neox-20b<span class="w"> </span>--dtype<span class="w"> </span>float32<span class="w"> </span>--ipex<span class="w"> </span>--jit<span class="w"> </span>--ipex-weight-only-quantization
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
