diff --git a/src/transformers/utils/import_utils.py b/src/transformers/utils/import_utils.py
index f612a4e87..001675da3 100644
--- a/src/transformers/utils/import_utils.py
+++ b/src/transformers/utils/import_utils.py
@@ -273,7 +273,9 @@ def is_torch_sdpa_available():
     # - Allow the global use of the `scale` argument introduced in https://github.com/pytorch/pytorch/pull/95259
     # - Memory-efficient attention supports arbitrary attention_mask: https://github.com/pytorch/pytorch/pull/104310
     # NOTE: We require torch>=2.1.1 to avoid a numerical issue in SDPA with non-contiguous inputs: https://github.com/pytorch/pytorch/issues/112577
-    return version.parse(_torch_version) >= version.parse("2.1.1")
+    print("Skip torch version check for xpu SDPA!")
+    return True
+    # return version.parse(_torch_version) >= version.parse("2.1.1")
 
 
 def is_torchvision_available():
