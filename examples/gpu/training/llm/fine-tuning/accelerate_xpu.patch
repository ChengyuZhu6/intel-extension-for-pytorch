diff --git a/src/accelerate/accelerator.py b/src/accelerate/accelerator.py
index 2815a00..bad3737 100755
--- a/src/accelerate/accelerator.py
+++ b/src/accelerate/accelerator.py
@@ -306,7 +306,11 @@ class Accelerator:
             fsdp_plugin, FullyShardedDataParallelPlugin
         ):
             if is_torch_version("<", FSDP_PYTORCH_VERSION):
-                raise ValueError(f"FSDP requires PyTorch >= {FSDP_PYTORCH_VERSION}")
+                from packaging.version import parse
+                import importlib.metadata
+                torch_version = parse(importlib.metadata.version("torch"))
+                print("Skip xpu FSDP version check!")
+                # raise ValueError(f"FSDP requires PyTorch >= {FSDP_PYTORCH_VERSION}")
 
         if fsdp_plugin is None:  # init from env variables
             fsdp_plugin = (
diff --git a/src/accelerate/utils/fsdp_utils.py b/src/accelerate/utils/fsdp_utils.py
index edff9de..32cd162 100644
--- a/src/accelerate/utils/fsdp_utils.py
+++ b/src/accelerate/utils/fsdp_utils.py
@@ -21,7 +21,8 @@ from .imports import is_torch_distributed_available
 from .versions import is_torch_version
 
 
-if is_torch_version(">=", FSDP_PYTORCH_VERSION) and is_torch_distributed_available():
+# if is_torch_version(">=", FSDP_PYTORCH_VERSION) and is_torch_distributed_available():
+if is_torch_distributed_available():
     import torch.distributed.checkpoint as dist_cp
     from torch.distributed.checkpoint.default_planner import DefaultLoadPlanner, DefaultSavePlanner
     from torch.distributed.checkpoint.optimizer import load_sharded_optimizer_state_dict
